{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56cbb3cf",
   "metadata": {},
   "source": [
    "# Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97199bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install datasets\n",
    "# !pip install wordninja\n",
    "# !pip install textblob\n",
    "# !pip install nltk\n",
    "# !pip install sentence-transformers\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# !pip install swifter\n",
    "# !pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e55d2",
   "metadata": {},
   "source": [
    "# Import Libraries + Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4100a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import re\n",
    "import swifter\n",
    "import gensim.downloader as api\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import string\n",
    "import wordninja\n",
    "import warnings\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import Word\n",
    "from nltk.corpus import wordnet\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# suppress UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "# to convert sentences to vector # runs fast\n",
    "sent2vec_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# to convert word to vector\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd9ed06",
   "metadata": {},
   "source": [
    "# Fetch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37aef4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'relation'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'relation'],\n",
       "        num_rows: 2717\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"sem_eval_2010_task_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b93c7",
   "metadata": {},
   "source": [
    "NOTE: This dataset doesnot contain any validation set, hence validation is splitting the training set"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ede4fd4e",
   "metadata": {},
   "source": [
    "Training Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9da2bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(load_dataset(\"sem_eval_2010_task_8\", split = \"train\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b10753ca",
   "metadata": {},
   "source": [
    "Testing Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e076c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(load_dataset(\"sem_eval_2010_task_8\", split = \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00849637",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a31998d",
   "metadata": {},
   "source": [
    "Functions to extract features, improve data quality and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48797df4",
   "metadata": {},
   "source": [
    "### 1. Extract Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc22f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc317e",
   "metadata": {},
   "source": [
    "Sentence contains entities enclosed in '<>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16698a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(sentence):\n",
    "    try:\n",
    "        e1 = re.search(r'<e1>(.*?)</e1>', sentence).group(1).lower().strip()\n",
    "        e2 = re.search(r'<e2>(.*?)</e2>', sentence).group(1).lower().strip()\n",
    "    except:\n",
    "        # raise error if entities are not enclosed in '<>' in sentence\n",
    "        raise ValueError('Sentence passed is not in correct format')\n",
    "    return pd.Series([e1, e2], index=['e1', 'e2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b607b1",
   "metadata": {},
   "source": [
    "### 2. Text to Vectors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b577551",
   "metadata": {},
   "source": [
    "Pre-trained word2vec module was implemented to convert text to vectors using following checks:\n",
    "1. Basic text was searched in word2vec pre-trained model\n",
    "2. If not found, text was capitalsied and then searched \n",
    "3. If not found, hyphenated texts were unhyphenated and then searched\n",
    "4. If not found, texts are converted into base form (singular, present tense) and then searched\n",
    "5. If not found, and there exists more than one word, the last word is searched\n",
    "6. If not found, and there exists more than one word, the first word is searched\n",
    "7. If not found, some rule-based word tunings are applied and check (removed 'er' at the end, removed 'ment' at the end)\n",
    "8. If not found, spelling mistakes are corrected, if any, and then searched\n",
    "9. If not found, the words are broken into segments where both words make sense (eg. floodwaters -> flood and waters) and the longer word, then last word and then first word is searched\n",
    "10. If still not found, zero vector is assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14760d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_checker = SpellChecker(distance=1)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "count = 0\n",
    "\n",
    "def text_to_vectors(text,sentence='',flag=0):\n",
    "    global count\n",
    "    flag = 0\n",
    "    return_text = ''\n",
    "    \n",
    "    try:\n",
    "        text = int(text[0])\n",
    "        text = 'number'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if text == None:\n",
    "        return_vector = text\n",
    "        \n",
    "    elif text in word2vec_model: # 1. search text in word2vec\n",
    "        return_text = text\n",
    "    \n",
    "    elif text.capitalize() in word2vec_model: # 2. search capitalised text in word2vec\n",
    "        return_text = text.capitalize()\n",
    "        \n",
    "    elif text.replace('-','') in word2vec_model: # 3. remove hyphen and search\n",
    "        return_text = text.replace('-','')\n",
    "        \n",
    "    elif str(lemmatizer.lemmatize(text, pos='n')) in word2vec_model: # 4. convert to singular and then search\n",
    "        return_text = str(lemmatizer.lemmatize(text, pos='n'))\n",
    "        \n",
    "    elif str(lemmatizer.lemmatize(text, pos='n')).capitalize() in word2vec_model: # 5. convert to singular & capitalise and then search\n",
    "        return_text = str(lemmatizer.lemmatize(text, pos='n')).capitalize()\n",
    "        \n",
    "    elif str(Word(text).lemmatize()) in word2vec_model: # 6. convert past to present and search\n",
    "        return_text = str(Word(text).lemmatize())\n",
    "    \n",
    "    elif text.replace('-',' ').split()[-1] in word2vec_model: # 7. if >1 words, search last word\n",
    "        return_text = text.replace('-',' ').split()[-1]\n",
    "        \n",
    "    elif text.replace('-',' ').split()[0] in word2vec_model: # 8. if >1 words, search first word\n",
    "        return_text = text.replace('-',' ').split()[0]\n",
    "        \n",
    "    # some custom defined rules\n",
    "    # 1. 'er' cases\n",
    "    elif str(Word(text).lemmatize())[-2:]=='er' and str(Word(text).lemmatize())[:-2] in word2vec_model: # 9. remove last 'er' of word and search\n",
    "        return_text = str(Word(text).lemmatize())[:-2]\n",
    "        \n",
    "    # 2. 'ment' cases\n",
    "    elif str(Word(text).lemmatize())[-4:]=='ment' and str(Word(text).lemmatize())[:-4] in word2vec_model: # 10. remove last 'ment' of word and search\n",
    "        return_text = str(Word(text).lemmatize())[:-4]\n",
    "    \n",
    "    elif spell_checker.correction(text) in word2vec_model: # 11. correct spelling, if any, and search\n",
    "        return_text = spell_checker.correction(text)\n",
    "    \n",
    "    elif wordninja.split(nlp(text)[0].lemma_)!=[] and max(wordninja.split(nlp(text)[0].lemma_), key=len) in word2vec_model: # 12. split words into segment and search the longest word\n",
    "        return_text = max(wordninja.split(nlp(text)[0].lemma_), key=len)\n",
    "\n",
    "    elif wordninja.split(nlp(text)[0].lemma_)!=[] and str(wordninja.split(nlp(text)[0].lemma_)[-1]) in word2vec_model: # 13. split words into segment and search the last word\n",
    "        return_text = str(wordninja.split(nlp(text)[0].lemma_)[-1])\n",
    "    \n",
    "    elif wordninja.split(nlp(text)[0].lemma_)!=[] and str(wordninja.split(nlp(text)[0].lemma_)[0]) in word2vec_model: # 14. split words into segment and search the first word\n",
    "        return_text = str(wordninja.split(nlp(text)[0].lemma_)[0])\n",
    "\n",
    "    else:\n",
    "        print ('Word:\"'+text+'\" NOT FOUND')\n",
    "        pass    \n",
    "    \n",
    "    if return_text =='':\n",
    "        count = count+1\n",
    "        flag = flag+1\n",
    "        return_text = text\n",
    "        return_vector = np.zeros(word2vec_model.vector_size)\n",
    "    elif return_text == None:\n",
    "        return_vector = np.zeros(word2vec_model.vector_size)\n",
    "    else:\n",
    "        return_vector = word2vec_model[return_text]\n",
    "        \n",
    "    if sentence=='':\n",
    "        return return_vector\n",
    "    else:\n",
    "        return_sentence = sentence.replace(text, return_text)\n",
    "        match = re.search(r\"<e1>(.*?)</e2>\", return_sentence)\n",
    "        if match:\n",
    "            trimmed_sentence = (match.group(1))  # trim sentence between 2 entities\n",
    "        else:\n",
    "            trimmed_sentence = None\n",
    "        # remove entity tags '<>'\n",
    "        return_sentence = re.sub(r\"<[^>]+>\", \"\",return_sentence)\n",
    "        trimmed_sentence = re.sub(r\"<[^>]+>\", \"\",trimmed_sentence)\n",
    "        # remove punctuations\n",
    "        punctuation_set = ''.join(char for char in string.punctuation)\n",
    "        return_sentence = return_sentence.translate(str.maketrans('', '', punctuation_set))\n",
    "        trimmed_sentence = trimmed_sentence.translate(str.maketrans('', '', punctuation_set))\n",
    "        # remove extra spaces\n",
    "        return_sentence = re.sub(r\"\\s+\", \" \", return_sentence).strip()\n",
    "        trimmed_sentence = re.sub(r\"\\s+\", \" \", trimmed_sentence).strip()\n",
    "        \n",
    "    \n",
    "        return pd.Series([return_vector,flag, return_sentence, return_text, trimmed_sentence], index=['e_vector', 'flag', 'corrected_sentence', 'e', 'trimmed_sentence'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed09f7",
   "metadata": {},
   "source": [
    "### 3. Custom Entity Disambiguation "
   ]
  },
  {
   "cell_type": "raw",
   "id": "16cbecdc",
   "metadata": {},
   "source": [
    "Extract the actual meaning of entities in the sentence (contextual meaning) using custom logic\n",
    "1. Fetch the synsets of the entities (broader categories)\n",
    "2. Extract the definitions of the synsets\n",
    "3. Find the cosine similarity of the main words (adjectives, nouns and verbs) in the definition and the sentence (excluding the entity word)\n",
    "4. The synset definition with most similarity with sentence is selected as the synset\n",
    "5. Once both entities synset have been derived, their lowest common hypernym is also extract to understand their relationship"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42ff548a",
   "metadata": {},
   "source": [
    "1. SYNSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1607606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_synsets(word):\n",
    "    words = word.replace('-',' ').split(' ')\n",
    "    new_words = []\n",
    "    \n",
    "    # segment words and check for synset\n",
    "    # 1.\n",
    "    for item in words:\n",
    "        new_words = new_words + wordninja.split(nlp(item)[0].lemma_)\n",
    "     \n",
    "    # 2.\n",
    "    for j in words:\n",
    "        for i in range(1,len(j)):\n",
    "            prefix = j[:i]\n",
    "            suffix = j[i:]\n",
    "            if prefix in word2vec_model and suffix in word2vec_model:\n",
    "\n",
    "                new_words = [prefix,suffix] + new_words\n",
    "            elif prefix in word2vec_model and len(prefix)>=5:\n",
    "                new_words = [prefix] + new_words\n",
    "            elif suffix in word2vec_model and len(suffix)>=5:\n",
    "                new_words = [suffix] + new_words\n",
    "\n",
    "    new_words = [item for item in new_words if len(item) >= 3]\n",
    "        \n",
    "    synsets = []\n",
    "    for item in new_words:\n",
    "        item_synset = wordnet.synsets(item)\n",
    "        synsets = synsets + item_synset\n",
    "    return synsets\n",
    "\n",
    "    \n",
    "\n",
    "def filter_main_tokens(sentence):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    # filtering based on POS tags\n",
    "    filtered_tokens = [token.text for token in doc if token.pos_ in (\"NOUN\", \"VERB\", \"ADJ\")]\n",
    "\n",
    "    return filtered_tokens\n",
    "    \n",
    "\n",
    "def disambiguate_entity_in_sentence(sentence, word, flag):\n",
    "    \n",
    "    # main tokens of the sentence\n",
    "    sentence_tokens = filter_main_tokens(sentence.replace(word,''))\n",
    "    \n",
    "    # fetch synsets for the word\n",
    "    synsets = wordnet.synsets(word.replace(' ','_'))\n",
    "    \n",
    "    # handle british vs american english\n",
    "    if len(synsets)==0 and word.count('s')==1:\n",
    "        synsets = wordnet.synsets(word.replace('s','z'))\n",
    "    if len(synsets)==0 and word.count('z')==1:\n",
    "        synsets = wordnet.synsets(word.replace('z','s'))\n",
    "        \n",
    "    # custom synset extraction by segmenting words\n",
    "    if len(synsets)==0:\n",
    "        synsets = segment_synsets(word)\n",
    "    \n",
    "    if len(synsets)==0:\n",
    "        print ('Synset for \"'+word+'\" NOT FOUND')  \n",
    "    \n",
    "    # fetch the most relatable synset based on sentence\n",
    "    scores = {}\n",
    "    for synset in synsets:\n",
    "        synset_tokens = filter_main_tokens(synset.definition())\n",
    "\n",
    "        synset_embeddings = [word2vec_model[token] for token in synset_tokens if token in word2vec_model]\n",
    "        \n",
    "        if len(synset_embeddings)>0:\n",
    "            avg_synset_embedding = np.mean(synset_embeddings, axis=0)\n",
    "            \n",
    "            # calculate similarity score based on cosine similarity between avg_synset_embedding and each token in sentence\n",
    "            similarity_scores = [np.dot(avg_synset_embedding, word2vec_model[token])/(np.linalg.norm(avg_synset_embedding)*np.linalg.norm(word2vec_model[token]))\n",
    "                                 for token in sentence_tokens if token in word2vec_model]\n",
    "            scores[synset] = np.mean(similarity_scores)\n",
    "        \n",
    "        else:\n",
    "            scores[synset] = 0\n",
    "    \n",
    "    # extract the highest score synset\n",
    "    if len(synsets)>0:\n",
    "        best_synset = max(scores, key=scores.get)\n",
    "        \n",
    "    else:\n",
    "        best_synset = wordnet.synsets('unavailable')[0]\n",
    "        flag = flag+1 #update warning flag\n",
    "    e_definition = best_synset.definition()\n",
    "    \n",
    "    return pd.Series([best_synset, e_definition, flag], index=['e_synset', 'e_definition', 'flag'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79715794",
   "metadata": {},
   "source": [
    "2. HYPERNYM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0848b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernym(synset1, synset2):\n",
    "    common_hypernym = synset1.lowest_common_hypernyms(synset2)\n",
    "    return common_hypernym[0].lemmas()[0].name() if common_hypernym else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0858c4",
   "metadata": {},
   "source": [
    "### 4. Extract Features"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9eef8817",
   "metadata": {},
   "source": [
    "Extra features wrt entities:\n",
    "    1. entities POS tag\n",
    "    2. dependency parsing: entity dependent token and their pos tag\n",
    "    3. tokens before and after entities\n",
    "    4. words between entities: total words and pos tags\n",
    "    5. entities position: start and end (based on letters and tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff658c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(e1, e2, sentence):\n",
    "\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    e1_pos, e2_pos = None, None\n",
    "    e1_dep_token, e2_dep_token = 'NA.', 'NA.'\n",
    "    e1_prev_token, e2_prev_token = 'NA.', 'NA.'\n",
    "    e1_post_token, e2_post_token = 'NA.', 'NA.'\n",
    "    \n",
    "    e1_dep_noun, e1_dep_adj, e1_dep_verb, e1_dep_prep, e1_dep_subj, e1_dep_obj = 0, 0, 0, 0, 0, 0\n",
    "    e2_dep_noun, e2_dep_adj, e2_dep_verb, e2_dep_prep, e2_dep_subj, e2_dep_obj = 0, 0, 0, 0, 0, 0\n",
    "    num_verbs, num_nouns, num_prep, num_adj, num_words_btwn = 0, 0, 0, 0, 0 \n",
    "    start_inbetween_count = False\n",
    "    \n",
    "    word_count = 0\n",
    "    e1_position_set = e2_position_set = False\n",
    "    e1_post_memory = e2_post_memory = False\n",
    "    memory = 'NA.'\n",
    "    e1_position = e2_position = -1\n",
    "    for token in doc:\n",
    "        word_count = word_count + 1\n",
    "        \n",
    "        if e1_post_memory == True:\n",
    "            e1_post_token = str(token)\n",
    "        e1_post_memory = False\n",
    "        \n",
    "        if e2_post_memory == True:\n",
    "            e2_post_token = str(token)\n",
    "        e2_post_memory = False\n",
    "        \n",
    "        # entity 1 dependency distribution\n",
    "        if str(token) in e1.split():\n",
    "            e1_pos = token.pos_\n",
    "            e1_dep_noun = 1 if token.dep_[0]=='n' else 0\n",
    "            e1_dep_adj = 1 if token.dep_[0]=='a' else 0\n",
    "            e1_dep_verb = 1 if token.dep_[0]=='v' else 0\n",
    "            e1_dep_prep = 1 if token.dep_[0]=='p' else 0\n",
    "            e1_dep_subj = 1 if 'subj' in token.dep_ else 0\n",
    "            e1_dep_obj = 1 if 'obj' in token.dep_ else 0\n",
    "            e1_dep_token = str(token.head)\n",
    "            \n",
    "            if e1_prev_token == 'NA.':\n",
    "                e1_prev_token = memory\n",
    "                \n",
    "            if e1_position_set == False:\n",
    "                e1_position = word_count\n",
    "                e1_position_set = True\n",
    "                \n",
    "            e1_post_memory = True\n",
    "            \n",
    "        # entity 2 dependency distribution    \n",
    "        if str(token) in e2.split(): \n",
    "            e2_pos = token.pos_\n",
    "            e2_dep_noun = 1 if token.dep_[0]=='n' else 0\n",
    "            e2_dep_adj = 1 if token.dep_[0]=='a' else 0\n",
    "            e2_dep_verb = 1 if token.dep_[0]=='v' else 0\n",
    "            e2_dep_prep = 1 if token.dep_[0]=='p' else 0\n",
    "            e2_dep_subj = 1 if 'subj' in token.dep_ else 0\n",
    "            e2_dep_obj = 1 if 'obj' in token.dep_ else 0\n",
    "            e2_dep_token = str(token.head)\n",
    "            \n",
    "            if e2_prev_token == 'NA.':\n",
    "                e2_prev_token = memory\n",
    "                \n",
    "            if e2_position_set == False:\n",
    "                e2_position = word_count\n",
    "                e2_position_set = True\n",
    "                \n",
    "            e2_post_memory = True\n",
    "            \n",
    "        if (str(token) in [e1.split()[-1],e2.split()[-1]] and start_inbetween_count==False) or (str(token) in [e1.split()[0],e2.split()[0]] and start_inbetween_count==True):\n",
    "            start_inbetween_count = not(start_inbetween_count)\n",
    "        \n",
    "        # words distribution between entities\n",
    "        if start_inbetween_count==True:\n",
    "            num_verbs = num_verbs+1 if token.pos_=='VERB' else num_verbs\n",
    "            num_nouns = num_nouns+1 if token.pos_=='NOUN' else num_nouns\n",
    "            num_prep = num_prep+1 if token.pos_=='ADP' else num_prep\n",
    "            num_adj = num_adj+1 if token.pos_=='ADJ' else num_adj\n",
    "            num_words_btwn = num_words_btwn+1 \n",
    "            \n",
    "        memory = str(token)\n",
    "\n",
    "    # entity position in sentence\n",
    "    e1_start = sentence.find(e1)\n",
    "    e1_end = e1_start + len(e1)\n",
    "    e2_start = sentence.find(e2)\n",
    "    e2_end = e2_start + len(e2)\n",
    "\n",
    "    \n",
    "    return pd.Series([e1_pos, e1_dep_noun, e1_dep_adj, e1_dep_verb, e1_dep_prep, e1_dep_subj, e1_dep_obj, e1_dep_token, e1_prev_token, e1_post_token, e1_start, e1_end, e1_position, e2_pos, e2_dep_noun, e2_dep_adj, e2_dep_verb, e2_dep_prep, e2_dep_subj, e2_dep_obj, e2_dep_token, e2_prev_token, e2_post_token, e2_start, e2_end, e2_position, num_verbs, num_nouns, num_prep, num_adj, num_words_btwn], index=['e1_pos', 'e1_dep_noun', 'e1_dep_adj', 'e1_dep_verb', 'e1_dep_prep', 'e1_dep_subj', 'e1_dep_obj', 'e1_dep_token', 'e1_prev_token', 'e1_post_token', 'e1_start', 'e1_end', 'e1_position', 'e2_pos', 'e2_dep_noun', 'e2_dep_adj', 'e2_dep_verb', 'e2_dep_prep', 'e2_dep_subj', 'e2_dep_obj', 'e2_dep_token', 'e2_prev_token', 'e2_post_token', 'e2_start', 'e2_end', 'e2_position', 'num_verbs', 'num_nouns', 'num_prep', 'num_adj', 'num_words_btwn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2a418",
   "metadata": {},
   "source": [
    "### 5. Sentence Tuning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d0867c5",
   "metadata": {},
   "source": [
    "Select the important part of the sentence which focuses on the relation between entities\n",
    "1. Strip sentence between entity occurences only\n",
    "2. Include both the entities in the modified sentence\n",
    "3. Keep only the tokens having POS tags of verb, auxilliary verb and adjectives between the entities\n",
    "4. Add the meaning of the entities at the end of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba185b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_sentence(new_sentence, e1, e2, e1_def, e2_def):\n",
    "    \n",
    "    doc = nlp(new_sentence)\n",
    "    return_sentence = ''\n",
    "    prev_word_pos = ''\n",
    "    prev_word = ''\n",
    "    sentence_entry = False\n",
    "    \n",
    "    # strip sentence between entities and select important tokens\n",
    "    for word in doc:\n",
    "        if str(word) == e1 and sentence_entry == False:\n",
    "            sentence_entry = True\n",
    "            end_type = e2\n",
    "        if str(word) == e2 and sentence_entry == False:\n",
    "            sentence_entry = True\n",
    "            end_type = e1\n",
    "            \n",
    "        if (str(word) == e1 or str(word) == e2 or word.pos_ in ['ADP', 'AUX','VERB', 'X']) and sentence_entry==True:\n",
    "\n",
    "            if word.pos_ =='ADP' and not(prev_word_pos in ['AUX','VERB', 'X'] or prev_word == e1 or prev_word == e2):\n",
    "                pass\n",
    "            else:\n",
    "                return_sentence = return_sentence + ' ' + word.text\n",
    "            \n",
    "            prev_word_pos = word.pos_\n",
    "            prev_word = str(word)\n",
    "            \n",
    "            if end_type == str(word):\n",
    "                sentence_entry = False\n",
    "                break\n",
    "                \n",
    "    # extract entity definition tokens\n",
    "    def definition_tokens(doc):\n",
    "        new_def = ''\n",
    "        for word in doc:\n",
    "            if (word.pos_ in ['AUX','VERB', 'X', 'NOUN']):\n",
    "                new_def = new_def + ' ' + word.text\n",
    "        return new_def\n",
    "    \n",
    "    new_e1_def = definition_tokens(nlp(e1_def))\n",
    "    new_e2_def = definition_tokens(nlp(e2_def))\n",
    "                  \n",
    "    return pd.Series([return_sentence[1:], return_sentence[1:].strip()+' where '+e1+' is the '+new_e1_def.strip()+' and '+e2+' is the '+new_e2_def.strip()], index=['custom_sentence', 'corrected_sentence'])\n",
    "                                                                                                                                          \n",
    "                                                                                                                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e3f86",
   "metadata": {},
   "source": [
    "### Apply All Functions (1. + 2. + 3. + 4. + 5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8efc0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_original, train=True):\n",
    "    df = df_original.copy()\n",
    "    df[['e1','e2']] = df['sentence'].swifter.apply(extract_entities)\n",
    "    df['warning_flags'] = 0\n",
    "    df[['e1_vector','warning_flags','corrected_sentence','corrected_e1','trimmed_sentence']] = df.swifter.apply(lambda x: text_to_vectors(x['e1'],x['sentence'],x['warning_flags']),axis=1)\n",
    "    df[['e2_vector','warning_flags','corrected_sentence','corrected_e2','trimmed_sentence']] = df.swifter.apply(lambda x: text_to_vectors(x['e2'],x['sentence'],x['warning_flags']),axis=1)\n",
    "    df[['e1_synset','e1_definition','warning_flags']] = df.swifter.apply(lambda x: disambiguate_entity_in_sentence(x['corrected_sentence'],x['corrected_e1'],x['warning_flags']), axis=1)\n",
    "    df[['e2_synset','e2_definition','warning_flags']] = df.swifter.apply(lambda x: disambiguate_entity_in_sentence(x['corrected_sentence'],x['corrected_e2'],x['warning_flags']), axis=1)\n",
    "    df['hypernym'] = df.swifter.apply(lambda x: get_hypernym(x['e1_synset'],x['e2_synset']),axis=1)\n",
    "    df[['e1_pos', 'e1_dep_noun', 'e1_dep_adj', 'e1_dep_verb', 'e1_dep_prep', 'e1_dep_subj', 'e1_dep_obj', 'e1_dep_token', 'e1_prev_token', 'e1_post_token', 'e1_start', 'e1_end', 'e1_position', 'e2_pos', 'e2_dep_noun', 'e2_dep_adj', 'e2_dep_verb', 'e2_dep_prep', 'e2_dep_subj', 'e2_dep_obj', 'e2_dep_token', 'e2_prev_token', 'e2_post_token', 'e2_start', 'e2_end', 'e2_position', 'num_verbs', 'num_nouns', 'num_prep', 'num_adj', 'num_words_btwn']] = df.swifter.apply(lambda x: extract_features(x['corrected_e1'],x['corrected_e2'],x['corrected_sentence']), axis=1)\n",
    "    df[['custom_sentence','custom_sentence_wdef']] = df.swifter.apply(lambda x: tune_sentence(x['trimmed_sentence'],x['corrected_e1'],x['corrected_e2'],x['e1_definition'],x['e2_definition']),axis=1)\n",
    "    \n",
    "    df['hypernym'] = df.swifter.apply(lambda x: text_to_vectors(x['hypernym']),axis=1)\n",
    "    df['e1_prev_token'] = df.swifter.apply(lambda x: text_to_vectors(x['e1_prev_token']),axis=1)\n",
    "    df['e2_prev_token'] = df.swifter.apply(lambda x: text_to_vectors(x['e2_prev_token']),axis=1)\n",
    "    df['e1_post_token'] = df.swifter.apply(lambda x: text_to_vectors(x['e1_post_token']),axis=1)\n",
    "    df['e2_post_token'] = df.swifter.apply(lambda x: text_to_vectors(x['e2_post_token']),axis=1)\n",
    "    df['e1_dep_token'] = df.swifter.apply(lambda x: text_to_vectors(x['e1_dep_token']),axis=1)\n",
    "    df['e2_dep_token'] = df.swifter.apply(lambda x: text_to_vectors(x['e2_dep_token']),axis=1)\n",
    "    \n",
    "    if train==True:\n",
    "        df = df[df['warning_flags']==0]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3b0e7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb46d1656134ac2b6a86f9d208158d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b77d439a86e42ffa6a0d1d428c92449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:\"mesencephalon\" NOT FOUND\n",
      "Word:\"azeotrope\" NOT FOUND\n",
      "Word:\"tokotoko\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e62a7cfc638431699ec8c342d131744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:\"mesophyll\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880e74809bf246509d9528bdff438b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset for \"geoid\" NOT FOUND\n",
      "Synset for \"mellitus\" NOT FOUND\n",
      "Synset for \"nye\" NOT FOUND\n",
      "Synset for \"helicobacter\" NOT FOUND\n",
      "Synset for \"Pleiad\" NOT FOUND\n",
      "Synset for \"chai\" NOT FOUND\n",
      "Synset for \"Pleiad\" NOT FOUND\n",
      "Synset for \"santur\" NOT FOUND\n",
      "Synset for \"Cavvy\" NOT FOUND\n",
      "Synset for \"configurator\" NOT FOUND\n",
      "Synset for \"opioids\" NOT FOUND\n",
      "Synset for \"Pleiad\" NOT FOUND\n",
      "Synset for \"Pleiad\" NOT FOUND\n",
      "Synset for \"Wunch\" NOT FOUND\n",
      "Synset for \"Pleiad\" NOT FOUND\n",
      "Synset for \"Sord\" NOT FOUND\n",
      "Synset for \"vulvodynia\" NOT FOUND\n",
      "Synset for \"app\" NOT FOUND\n",
      "Synset for \"exe\" NOT FOUND\n",
      "Synset for \"rar\" NOT FOUND\n",
      "Synset for \"joey\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82827a12be8b4f68bee2b029afd5e1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset for \"depo\" NOT FOUND\n",
      "Synset for \"mai\" NOT FOUND\n",
      "Synset for \"apps\" NOT FOUND\n",
      "Synset for \"mesophyll\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee73bbfaa924ea6a567d6da65706ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2575ba54dc24ab5af1ce44b497661c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d43a5f35f647b7afca90224c753851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adc8dcf528c4235a989b4247e57c8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4c6b93352448b1b819caf0155d561b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:\"Ibsens\" NOT FOUND\n",
      "Word:\"Cassiolis\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dde7b9a4c7649028d76ca5e31a64bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:\"Neuraths\" NOT FOUND\n",
      "Word:\"oxalic\" NOT FOUND\n",
      "Word:\"Middlesbroughs\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23b6847bf484182b908eb236470552c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490c1285b5dc4958b2e62a51cf98eb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fa34d7236640d59f8f04dc7fd12fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ffb15aa1724deeb8c76403317008de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b4b0fe7114481f86b41c3b6bea725f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05f272aa1bc410bb597a9fe34ea1a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b763715240940d4885dad3702c69010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4ad483f02345e3816c1cb667988cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset for \"app\" NOT FOUND\n",
      "Synset for \"cha\" NOT FOUND\n",
      "Synset for \"sid\" NOT FOUND\n",
      "Synset for \"Dule\" NOT FOUND\n",
      "Synset for \"Sord\" NOT FOUND\n",
      "Synset for \"Dule\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7897d4ccabe846edb0874a1fa3bc4a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset for \"yourselfer\" NOT FOUND\n",
      "Synset for \"wiki\" NOT FOUND\n",
      "Synset for \"prequels\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae4ee9307cc4438932d04563d14c071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f18a51e0ae44919bc8a0594caa287b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d086d0c4d5f4d97a5c611f85fd1ee34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde868c9401a4839ae382b41a8f93bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c47f9ed09394590897b2bb3cdddfa61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:\"hammerbeam\" NOT FOUND\n",
      "Word:\"Hackmans\" NOT FOUND\n",
      "Word:\"Newsteds\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1212659d6a44e7a88cdc02be3a74c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:\"abyssinica\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b57c861d684228856ac3f6e4fd6924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:\"ploughed\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83252143898d4a9a87f3fc0827d82d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe5c298c41940dd9597d5e320a95690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:\"ploughed\" NOT FOUND\n",
      "Word:\"manoeuvred\" NOT FOUND\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585a90c07575491a85c7340a40d36416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = preprocess_data(train_df)\n",
    "test_df = preprocess_data(test_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7dc80",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42490ff4",
   "metadata": {},
   "source": [
    "### 1. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c01f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(df,bow=False, *args):\n",
    "\n",
    "    train_data = None\n",
    "    \n",
    "    for feature in args:\n",
    "        if isinstance(df[feature].iloc[0], np.ndarray):\n",
    "            if train_data is None:  \n",
    "                train_data = np.vstack(df[feature])\n",
    "            else:\n",
    "                train_data = np.concatenate((train_data, np.vstack(df[feature])),axis=1)\n",
    "\n",
    "        elif isinstance(df[feature].iloc[0], np.int64):\n",
    "            if train_data is None:  \n",
    "                train_data = np.array(df[feature]).reshape(-1, 1)\n",
    "            else:\n",
    "                train_data = np.concatenate((train_data, np.array(df[feature]).reshape(-1, 1)),axis=1)\n",
    "                           \n",
    "        elif isinstance(df[feature].iloc[0], str):\n",
    "\n",
    "            if train_data is None:  \n",
    "                train_data = np.vstack(sent2vec_model.encode(df[feature].tolist()))\n",
    "            else:\n",
    "                train_data = np.concatenate((train_data, np.vstack(sent2vec_model.encode(df[feature].tolist()))),axis=1) \n",
    "                           \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    if bow is not None and len(bow)>0:\n",
    "        if train_data is None:  \n",
    "            train_data = np.vstack(bow)\n",
    "        else:\n",
    "            train_data = np.concatenate((train_data, np.vstack(bow)),axis=1)\n",
    "    \n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388aadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(df,feature):\n",
    "    return np.array(df[feature].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb247228",
   "metadata": {},
   "source": [
    "### 2. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b587e8",
   "metadata": {},
   "source": [
    "### Experiment 1: Custom Sentence and Entity Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5b802ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70        55\n",
      "           1       0.76      0.86      0.81        91\n",
      "           2       0.74      0.73      0.73        70\n",
      "           3       0.49      0.66      0.56        61\n",
      "           4       0.74      0.78      0.76        65\n",
      "           5       0.90      0.72      0.80        25\n",
      "           6       0.72      0.70      0.71       119\n",
      "           8       0.67      0.70      0.68        77\n",
      "           9       0.93      0.81      0.86        31\n",
      "          10       0.56      0.69      0.62        13\n",
      "          11       0.59      0.67      0.63        61\n",
      "          12       0.44      0.50      0.47         8\n",
      "          13       0.77      0.87      0.81        76\n",
      "          14       0.71      0.74      0.72        73\n",
      "          15       0.86      0.63      0.73        19\n",
      "          16       0.84      0.69      0.76        52\n",
      "          17       0.74      0.60      0.66        67\n",
      "          18       0.52      0.46      0.49       234\n",
      "\n",
      "    accuracy                           0.68      1197\n",
      "   macro avg       0.70      0.70      0.69      1197\n",
      "weighted avg       0.68      0.68      0.67      1197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = train_df.drop(columns=['relation']) \n",
    "y = train_df['relation']  \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# add bag of words feature\n",
    "# add features for words occuring frequently: min 25 times\n",
    "vectorizer = CountVectorizer(min_df=25)\n",
    "train_bow = vectorizer.fit_transform(X_train['custom_sentence']).toarray()\n",
    "val_bow = vectorizer.transform(X_val['custom_sentence']).toarray()\n",
    "\n",
    "\n",
    "X_train = get_X(X_train, train_bow, 'custom_sentence_wdef','e1_vector','e2_vector')\n",
    "X_val = get_X(X_val, val_bow, 'custom_sentence_wdef','e1_vector','e2_vector')\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f81128",
   "metadata": {},
   "source": [
    "### Experiment 2: Custom Sentence, Entity Vectors and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d747edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80        55\n",
      "           1       0.84      0.87      0.85        91\n",
      "           2       0.62      0.79      0.69        70\n",
      "           3       0.64      0.70      0.67        61\n",
      "           4       0.82      0.75      0.78        65\n",
      "           5       0.83      0.76      0.79        25\n",
      "           6       0.84      0.85      0.85       119\n",
      "           8       0.74      0.78      0.76        77\n",
      "           9       0.93      0.84      0.88        31\n",
      "          10       0.60      0.46      0.52        13\n",
      "          11       0.68      0.80      0.74        61\n",
      "          12       0.86      0.75      0.80         8\n",
      "          13       0.78      0.88      0.83        76\n",
      "          14       0.74      0.85      0.79        73\n",
      "          15       0.75      0.63      0.69        19\n",
      "          16       0.89      0.75      0.81        52\n",
      "          17       0.74      0.67      0.70        67\n",
      "          18       0.56      0.47      0.51       234\n",
      "\n",
      "    accuracy                           0.73      1197\n",
      "   macro avg       0.76      0.75      0.75      1197\n",
      "weighted avg       0.73      0.73      0.73      1197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = train_df.drop(columns=['relation']) \n",
    "y = train_df['relation']  \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# add bag of words feature\n",
    "# add features for words occuring frequently: min 25 times\n",
    "vectorizer = CountVectorizer(min_df=25)\n",
    "train_bow = vectorizer.fit_transform(X_train['custom_sentence']).toarray()\n",
    "val_bow = vectorizer.transform(X_val['custom_sentence']).toarray()\n",
    "\n",
    "\n",
    "X_train = get_X(X_train, train_bow, 'custom_sentence_wdef','e1_vector','e2_vector','e1_dep_token','e2_dep_token','e1_prev_token','e2_prev_token','e1_post_token','e2_post_token')\n",
    "X_val = get_X(X_val, val_bow, 'custom_sentence_wdef','e1_vector','e2_vector','e1_dep_token','e2_dep_token','e1_prev_token','e2_prev_token','e1_post_token','e2_post_token')\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57919f",
   "metadata": {},
   "source": [
    "### Experiment 3: Custom Sentence, Entity Vectors and Dependencies + Additional Definition & Hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ae20e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80        55\n",
      "           1       0.84      0.86      0.85        91\n",
      "           2       0.65      0.79      0.71        70\n",
      "           3       0.69      0.67      0.68        61\n",
      "           4       0.80      0.80      0.80        65\n",
      "           5       0.87      0.80      0.83        25\n",
      "           6       0.84      0.87      0.85       119\n",
      "           8       0.72      0.74      0.73        77\n",
      "           9       0.90      0.87      0.89        31\n",
      "          10       0.73      0.62      0.67        13\n",
      "          11       0.70      0.82      0.76        61\n",
      "          12       0.86      0.75      0.80         8\n",
      "          13       0.78      0.88      0.83        76\n",
      "          14       0.73      0.84      0.78        73\n",
      "          15       0.74      0.74      0.74        19\n",
      "          16       0.82      0.77      0.79        52\n",
      "          17       0.70      0.70      0.70        67\n",
      "          18       0.59      0.47      0.52       234\n",
      "\n",
      "    accuracy                           0.74      1197\n",
      "   macro avg       0.76      0.77      0.76      1197\n",
      "weighted avg       0.73      0.74      0.73      1197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = train_df.drop(columns=['relation']) \n",
    "y = train_df['relation']  \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# add bag of words feature\n",
    "# add features for words occuring frequently: min 25 times\n",
    "vectorizer = CountVectorizer(min_df=25)\n",
    "train_bow = vectorizer.fit_transform(X_train['custom_sentence']).toarray()\n",
    "val_bow = vectorizer.transform(X_val['custom_sentence']).toarray()\n",
    "\n",
    "\n",
    "X_train = get_X(X_train, train_bow, 'custom_sentence_wdef','e1_vector','e2_vector','e1_dep_token','e2_dep_token','e1_prev_token','e2_prev_token','e1_post_token','e2_post_token','e1_definition','e2_definition','hypernym')\n",
    "X_val = get_X(X_val, val_bow, 'custom_sentence_wdef','e1_vector','e2_vector','e1_dep_token','e2_dep_token','e1_prev_token','e2_prev_token','e1_post_token','e2_post_token','e1_definition','e2_definition','hypernym')\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90c563",
   "metadata": {},
   "source": [
    "### Experiment 4: All Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7bded4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.82        55\n",
      "           1       0.80      0.88      0.84        91\n",
      "           2       0.67      0.76      0.71        70\n",
      "           3       0.68      0.69      0.68        61\n",
      "           4       0.83      0.75      0.79        65\n",
      "           5       0.83      0.76      0.79        25\n",
      "           6       0.88      0.88      0.88       119\n",
      "           8       0.75      0.78      0.76        77\n",
      "           9       0.89      0.81      0.85        31\n",
      "          10       0.67      0.62      0.64        13\n",
      "          11       0.74      0.82      0.78        61\n",
      "          12       0.75      0.75      0.75         8\n",
      "          13       0.80      0.91      0.85        76\n",
      "          14       0.72      0.85      0.78        73\n",
      "          15       0.78      0.74      0.76        19\n",
      "          16       0.91      0.77      0.83        52\n",
      "          17       0.69      0.75      0.72        67\n",
      "          18       0.58      0.47      0.52       234\n",
      "\n",
      "    accuracy                           0.74      1197\n",
      "   macro avg       0.76      0.77      0.76      1197\n",
      "weighted avg       0.74      0.74      0.74      1197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = train_df.drop(columns=['relation']) \n",
    "y = train_df['relation']  \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# add bag of words feature\n",
    "# add features for words occuring frequently: min 25 times\n",
    "vectorizer = CountVectorizer(min_df=25)\n",
    "train_bow = vectorizer.fit_transform(X_train['custom_sentence']).toarray()\n",
    "val_bow = vectorizer.transform(X_val['custom_sentence']).toarray()\n",
    "\n",
    "\n",
    "X_train = get_X(X_train, train_bow, 'custom_sentence_wdef','e1_vector','e2_vector', 'e1_dep_noun', 'e1_dep_adj', 'e1_dep_verb', 'e1_dep_prep', 'e1_dep_subj', 'e1_dep_obj', 'e1_dep_token', 'e1_prev_token', 'e1_post_token', 'e1_start', 'e1_end', 'e1_position', 'e2_dep_noun', 'e2_dep_adj', 'e2_dep_verb', 'e2_dep_prep', 'e2_dep_subj', 'e2_dep_obj', 'e2_dep_token', 'e2_prev_token', 'e2_post_token', 'e2_start', 'e2_end', 'e2_position', 'num_verbs', 'num_nouns', 'num_prep', 'num_adj', 'num_words_btwn', 'e1_definition', 'e2_definition','hypernym')\n",
    "X_val = get_X(X_val, val_bow, 'custom_sentence_wdef','e1_vector','e2_vector', 'e1_dep_noun', 'e1_dep_adj', 'e1_dep_verb', 'e1_dep_prep', 'e1_dep_subj', 'e1_dep_obj', 'e1_dep_token', 'e1_prev_token', 'e1_post_token', 'e1_start', 'e1_end', 'e1_position', 'e2_dep_noun', 'e2_dep_adj', 'e2_dep_verb', 'e2_dep_prep', 'e2_dep_subj', 'e2_dep_obj', 'e2_dep_token', 'e2_prev_token', 'e2_post_token', 'e2_start', 'e2_end', 'e2_position', 'num_verbs', 'num_nouns', 'num_prep', 'num_adj', 'num_words_btwn', 'e1_definition', 'e2_definition','hypernym')\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd9039",
   "metadata": {},
   "source": [
    "### Experiment 5: Best features + Excluding 'Other' type relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83e6614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86        63\n",
      "           1       0.94      0.85      0.89        92\n",
      "           2       0.77      0.88      0.82        64\n",
      "           3       0.81      0.72      0.76        82\n",
      "           4       0.86      0.86      0.86        63\n",
      "           5       1.00      1.00      1.00        22\n",
      "           6       0.88      0.94      0.91       119\n",
      "           8       0.82      0.82      0.82        76\n",
      "           9       0.90      0.90      0.90        20\n",
      "          10       1.00      0.67      0.80        18\n",
      "          11       0.82      0.82      0.82        57\n",
      "          12       1.00      0.73      0.84        11\n",
      "          13       0.89      0.97      0.93        91\n",
      "          14       0.87      0.90      0.89        84\n",
      "          15       0.73      0.69      0.71        16\n",
      "          16       0.85      0.92      0.89        51\n",
      "          17       0.87      0.81      0.84        57\n",
      "\n",
      "    accuracy                           0.86       986\n",
      "   macro avg       0.88      0.84      0.85       986\n",
      "weighted avg       0.86      0.86      0.86       986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = train_df[train_df['relation']!=18].drop(columns=['relation'])\n",
    "\n",
    "y = train_df[train_df['relation']!=18]['relation']  \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# add bag of words feature\n",
    "# add features for words occuring frequently: min 25 times\n",
    "vectorizer = CountVectorizer(min_df=25)\n",
    "train_bow = vectorizer.fit_transform(X_train['custom_sentence']).toarray()\n",
    "val_bow = vectorizer.transform(X_val['custom_sentence']).toarray()\n",
    "\n",
    "\n",
    "X_train = get_X(X_train, train_bow, 'custom_sentence_wdef','e1_vector','e2_vector','e1_dep_token','e2_dep_token','e1_prev_token','e2_prev_token','e1_post_token','e2_post_token','e1_definition','e2_definition','hypernym')\n",
    "X_val = get_X(X_val, val_bow, 'custom_sentence_wdef','e1_vector','e2_vector','e1_dep_token','e2_dep_token','e1_prev_token','e2_prev_token','e1_post_token','e2_post_token','e1_definition','e2_definition','hypernym')\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b740bfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print (len(train_df[train_df['relation']==7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e54c9e",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e24f59ff",
   "metadata": {},
   "source": [
    "1. Relation type 'Other' i.e. 18 has no specific pattern, and is hence the most difficult to predict. It hampers other category predictions as well, especially relation type 'Instrument-Agency(e1,e2)' i.e. 10. Without including others in training and validation accuracy is 86%.\n",
    "2. Only 1 training data is available for relation type 7. Hence, it has a very poor prediction.\n",
    "3. The categories with higher support tend to perform better which suggests a little more training examples could improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eae4fb",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33ece9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       134\n",
      "           1       0.88      0.90      0.89       194\n",
      "           2       0.70      0.77      0.73       162\n",
      "           3       0.66      0.71      0.68       150\n",
      "           4       0.81      0.83      0.82       153\n",
      "           5       0.90      0.69      0.78        39\n",
      "           6       0.84      0.88      0.86       291\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.81      0.82      0.81       211\n",
      "           9       0.84      0.79      0.81        47\n",
      "          10       0.53      0.41      0.46        22\n",
      "          11       0.80      0.78      0.79       134\n",
      "          12       0.77      0.62      0.69        32\n",
      "          13       0.83      0.87      0.85       201\n",
      "          14       0.79      0.82      0.80       210\n",
      "          15       0.73      0.65      0.69        51\n",
      "          16       0.81      0.72      0.76       108\n",
      "          17       0.83      0.73      0.78       123\n",
      "          18       0.52      0.50      0.51       454\n",
      "\n",
      "    accuracy                           0.76      2717\n",
      "   macro avg       0.73      0.71      0.72      2717\n",
      "weighted avg       0.76      0.76      0.76      2717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global vectorizer\n",
    "vectorizer = CountVectorizer(min_df=25)\n",
    "vectorizer = vectorizer.fit(train_df['custom_sentence'])\n",
    "train_bow = vectorizer.transform(train_df['custom_sentence']).toarray()\n",
    "test_bow = vectorizer.transform(test_df['custom_sentence']).toarray()\n",
    "\n",
    "\n",
    "\n",
    "X_train = get_X(train_df, train_bow, 'custom_sentence_wdef','e1_vector','e2_vector','e1_dep_token','e2_dep_token','e1_prev_token','e2_prev_token','e1_post_token','e2_post_token','e1_definition','e2_definition','hypernym')\n",
    "y_train = get_y(train_df, 'relation')\n",
    "X_test = get_X(test_df, test_bow, 'custom_sentence_wdef','e1_vector','e2_vector','e1_dep_token','e2_dep_token','e1_prev_token','e2_prev_token','e1_post_token','e2_post_token','e1_definition','e2_definition','hypernym')\n",
    "y_test = get_y(test_df, 'relation')\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# # Model evaluation\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f57681",
   "metadata": {},
   "source": [
    "# Inference Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45f67b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>relation type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Cause-Effect(e1,e2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Cause-Effect(e2,e1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Component-Whole(e1,e2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Component-Whole(e2,e1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Content-Container(e1,e2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Content-Container(e2,e1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Entity-Destination(e1,e2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Entity-Destination(e2,e1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Entity-Origin(e1,e2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Entity-Origin(e2,e1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Instrument-Agency(e1,e2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Instrument-Agency(e2,e1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Member-Collection(e1,e2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Member-Collection(e2,e1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Message-Topic(e1,e2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Message-Topic(e2,e1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Product-Producer(e1,e2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Product-Producer(e2,e1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    relation              relation type\n",
       "0          0        Cause-Effect(e1,e2)\n",
       "1          1        Cause-Effect(e2,e1)\n",
       "2          2     Component-Whole(e1,e2)\n",
       "3          3     Component-Whole(e2,e1)\n",
       "4          4   Content-Container(e1,e2)\n",
       "5          5   Content-Container(e2,e1)\n",
       "6          6  Entity-Destination(e1,e2)\n",
       "7          7  Entity-Destination(e2,e1)\n",
       "8          8       Entity-Origin(e1,e2)\n",
       "9          9       Entity-Origin(e2,e1)\n",
       "10        10   Instrument-Agency(e1,e2)\n",
       "11        11   Instrument-Agency(e2,e1)\n",
       "12        12   Member-Collection(e1,e2)\n",
       "13        13   Member-Collection(e2,e1)\n",
       "14        14       Message-Topic(e1,e2)\n",
       "15        15       Message-Topic(e2,e1)\n",
       "16        16    Product-Producer(e1,e2)\n",
       "17        17    Product-Producer(e2,e1)\n",
       "18        18                      Other"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relation_name = {\n",
    "0: 'Cause-Effect(e1,e2)',\n",
    "1: 'Cause-Effect(e2,e1)',\n",
    "2: 'Component-Whole(e1,e2)',\n",
    "3: 'Component-Whole(e2,e1)',\n",
    "4: 'Content-Container(e1,e2)',\n",
    "5: 'Content-Container(e2,e1)',\n",
    "6: 'Entity-Destination(e1,e2)',\n",
    "7: 'Entity-Destination(e2,e1)',\n",
    "8: 'Entity-Origin(e1,e2)',\n",
    "9: 'Entity-Origin(e2,e1)',\n",
    "10: 'Instrument-Agency(e1,e2)', \n",
    "11: 'Instrument-Agency(e2,e1)',\n",
    "12: 'Member-Collection(e1,e2)',\n",
    "13: 'Member-Collection(e2,e1)',\n",
    "14: 'Message-Topic(e1,e2)',\n",
    "15: 'Message-Topic(e2,e1)',\n",
    "16: 'Product-Producer(e1,e2)',\n",
    "17: 'Product-Producer(e2,e1)',\n",
    "18: 'Other'\n",
    "}\n",
    "\n",
    "display(pd.DataFrame([(key, value) for key, value in relation_name.items()], columns=[\"relation\", \"relation type\"]))\n",
    "\n",
    "def inference_mode(sentence):\n",
    "    if type(sentence)==str:\n",
    "        sentence = [sentence]\n",
    "    df = pd.DataFrame(sentence, columns = ['sentence'])\n",
    "    df = preprocess_data(df, False)\n",
    "\n",
    "    bow_d = vectorizer.transform(df['custom_sentence']).toarray()\n",
    "\n",
    "    X_test = get_X(df, bow_d, 'custom_sentence_wdef','e1_vector','e2_vector','e1_dep_token','e2_dep_token','e1_prev_token','e2_prev_token','e1_post_token','e2_post_token','e1_definition','e2_definition','hypernym')\n",
    "\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    df['predicted_relation'] = y_pred\n",
    "    df['predicted_relation_type'] = df['predicted_relation'].map(relation_name)\n",
    "    return df[['sentence','predicted_relation_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3070a2c5",
   "metadata": {},
   "source": [
    "### Try Your Sentence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ce14905",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "1. Input can be passed either as string or list of strings\n",
    "2. entites should be enclosed in the format shown: '<e1>Global warming</e1> is a result of <e2>deforestation</e2>.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d375dd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a613777e0f44d8abaf992816e649d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85bbc804b3b481abb5d95c32ee83276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a8903d8ba041349658295a78929660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1cab685e6e4137aff254797da6cd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4827d7def3e467cb0160cf2fe8e9a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e352c59c1a408a9be797f8d66e468d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ce71e20b3f40b395f7ca967b65036d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c2db531318493d850b1026e7b7db82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0db5a1faaa401a8a60d24907d9ef27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f19a16108846d0b2e62499cba62b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77500896a39441da8c6e1e74265883b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea66cd4916d45aba02ff47ccd59f296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf9e465827d4796bdedd0705187729d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a143aded1c584e0fbbb93e1496a155db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cebd51a77e14c4992073c5e6ed54059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>predicted_relation_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;e1&gt;Global warming&lt;/e1&gt; is a result of &lt;e2&gt;def...</td>\n",
       "      <td>Cause-Effect(e1,e2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence predicted_relation_type\n",
       "0  <e1>Global warming</e1> is a result of <e2>def...     Cause-Effect(e1,e2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_mode('<e1>Global warming</e1> is a result of <e2>deforestation</e2>.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
